# This takes sentences generated by generate_sentences.py and converts it to raw text
# While also keeping track of sentence lengths and creating a dictionary of words 
import os 
import nltk 
import json 

dictionary = {}
mapped_dictionary = {}
max_sentence_length = 0 
most_frequent_word = ""
most_frequent_word_freq = 0
total_words = 0 

# Build dictionary and convert sentences to raw text 
for filename in os.listdir(os.getcwd()+ "/data/json"):
    # Import data as a JSON object 
    data = []
    for line in open("data/json/" + filename, 'r'):
        data.append(json.loads(line))

    output_file = "data/txt/" + filename[:-5] + ".txt"
    open(output_file, 'w') # Clear contents of file 
    out = open(output_file, 'a+')

    for line in data:
        # Convert to raw text
        sentence = line['Arg1Raw'] + " " + line['ConnectiveRaw'] + " " + line['Arg2Raw'] + "\n"
        out.write(sentence) 

        # Tokenize sentence and build dictionary + corpus stats 
        word_sentence = nltk.word_tokenize(sentence.lower())
        if len(word_sentence) > max_sentence_length:
            max_sentence_length = len(word_sentence)
        for word in word_sentence:
            total_words += 1
            if word not in dictionary:
                dictionary[word] = 1
            else:
                dictionary[word] += 1 
                if dictionary[word] > most_frequent_word_freq:
                    most_frequent_word = word
                    most_frequent_word_freq = dictionary[word]

# Output dictionary and create mapping of terms to integers
index = 1
open("data/dictionary.txt", 'w') # Clear contents of file 
dict_file = open("data/dictionary.txt", 'a+')
# Used for padding sentences to max_sentence_length
mapped_dictionary["<pad>"] = 0
dict_file.write("0 <pad> -1\n")

# Print all terms from dictionary and map them to integers in mapped_dictionary
for key in sorted(dictionary.keys()):
    mapped_dictionary[key.lower()] = index
    entry = str(index) + " " + key + " " + str(dictionary[key]) + "\n"
    dict_file.write(entry)
    index += 1 

# Output corpus stats
open("data/corpus_stats.txt", 'w') # Clear contents of file 
stats_file = open("data/corpus_stats.txt", 'a+')
stats_file.write("Total words: " + str(total_words) + "\n")
stats_file.write("Most frequent word: " + str(most_frequent_word) + "\n")
stats_file.write("Most frequent word frequency: " + str(most_frequent_word_freq) + "\n")
stats_file.write("Unique terms in dictionary: " + str(len(dictionary.keys())) + "\n")
stats_file.write("Max sentence length: " + str(max_sentence_length) + "\n")

# Pad all sentences to max_sentence_length and covert sentences to integer representation
for filename in os.listdir(os.getcwd()+ "/data/txt"):
    # Import data as a JSON object 
    data = []
    for line in open("data/txt/" + filename, 'r'):
        tokenized_line = nltk.word_tokenize(line.lower())

        # Pad sentences
        while (len(tokenized_line) < max_sentence_length):
            tokenized_line.append("<pad>")
        data.append(tokenized_line)

    # Output padded sentences
    output_file = "data/padded/" + filename[:-4] + ".txt"
    open(output_file, 'w') # Clear contents of file 
    out = open(output_file, 'a+')

    for sentence in data:
        for word in sentence:
            out.write(word.lower() + " ")
        out.write("\n") 

    # Output integer sentences
    output_file = "data/integers/" + filename[:-4] + ".txt"
    open(output_file, 'w') # Clear contents of file 
    out = open(output_file, 'a+')    
    for sentence in data:
        for word in sentence:
            out.write(str(mapped_dictionary[word.lower()]) + " ")
        out.write("\n")