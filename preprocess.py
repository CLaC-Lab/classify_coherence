# This takes sentences generated by generate_sentences.py and converts it to raw text
# While also keeping track of sentence lengths and creating a dictionary of words 
import os 
import nltk 
import json 

dictionary = {}
mapped_dictionary = {}
max_sentence_length = 0 
most_frequent_word = ""
most_frequent_word_freq = 0
total_words = 0 

# Build dictionary and convert sentences to raw text 
for filename in os.listdir(os.getcwd()+ "/data/json"):
    # Import data as a JSON object 
    data = []
    for line in open("data/json/" + filename, 'r'):
        data.append(json.loads(line))

    output_file = "data/txt/" + filename[:-5] + ".txt"
    open(output_file, 'w') # Clear contents of file 
    out = open(output_file, 'a+')

    for line in data:
        sentence = line['Arg1Raw'] + " " + line['ConnectiveRaw'] + " " + line['Arg2Raw'] + "\n"
        out.write(sentence)

        word_sentence = nltk.word_tokenize(sentence.lower())
        if len(word_sentence) > max_sentence_length:
            max_sentence_length = len(word_sentence)
        for word in word_sentence:
            total_words += 1
            if word not in dictionary:
                dictionary[word] = 1
            else:
                dictionary[word] += 1 
                if dictionary[word] > most_frequent_word_freq:
                    most_frequent_word = word
                    most_frequent_word_freq = dictionary[word]

# Output dictionary and map words to integers
open("data/dictionary.txt", 'w') # Clear contents of file 
dict_file = open("data/dictionary.txt", 'a+')

index = 0 
for key in sorted(set(dictionary.keys())):
    mapped_dictionary[key] = index
    entry = str(index) + " " + key + " " + str(dictionary[key]) + "\n"
    dict_file.write(entry)
    index += 1 

# Output corpus stats
open("data/corpus_stats.txt", 'w') # Clear contents of file 
stats_file = open("data/corpus_stats.txt", 'a+')
stats_file.write("Total words: " + str(total_words) + "\n")
stats_file.write("Most frequent word: " + str(most_frequent_word) + "\n")
stats_file.write("Most frequent word frequency: " + str(most_frequent_word_freq) + "\n")
stats_file.write("Unique terms in dictionary: " + str(len(dictionary.keys())) + "\n")
stats_file.write("Max sentence length: " + str(max_sentence_length) + "\n")

# Pad all sentences to max_sentence_length and make all words lowercase
for filename in os.listdir(os.getcwd()+ "/data/txt"):
    # Import data as a JSON object 
    data = []
    for line in open("data/txt/" + filename, 'r'):
        tokenized_line = nltk.word_tokenize(line)
        while (len(tokenized_line) < max_sentence_length):
            tokenized_line.append("<PAD>")
        data.append(tokenized_line)

    output_file = "data/padded/" + filename[:-4] + ".txt"
    open(output_file, 'w') # Clear contents of file 
    out = open(output_file, 'a+')

    for sentence in data:
        for word in sentence:
            out.write(word.lower() + " ")
        out.write("\n") 

